---
title: "304 Final Paper"
author: "Yichun Zhang"
date: "4/23/2022"
output: html_document
---

```{r setup_lib, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
#install.packages("devtools")
#devtools::install_github("warint/statcanR")
library(tidyverse)
library(dplyr)
library(openintro)
library(janitor)
library(lubridate)....
```

# Introduction 

# Data 
```{r data_rent, include = FALSE}
library(opendatatoronto)
library(dplyr)

# get package
package <- show_package("4ef82789-e038-44ef-a478-a8f3590c3eb1")
package

# get all resources for this package
resources <- list_package_resources("4ef82789-e038-44ef-a478-a8f3590c3eb1")

# identify datastore resources; by default, Toronto Open Data sets datastore resource format to CSV for non-geospatial and GeoJSON for geospatial resources
datastore_resources <- filter(resources, tolower(format) %in% c('csv', 'geojson'))

# load the first datastore resource as a sample
data <- filter(datastore_resources, row_number()==1) %>% get_resource()
data
# Here you can load in and clean the data (you may need to do the cleaning in a separate R script). 

# You may need additional chunks, in case you want to include some of the cleaning output.

# Notice that the include=FALSE means that the code, and its resulting output, in this chunk will not appear in the pdf.
rent = data%>%mutate(id = c(1:nrow(data)),YEAR_BUILT = as.numeric(YEAR_BUILT),YEAR_EVALUATED = as.numeric(YEAR_EVALUATED), CONFIRMED_STOREYS = as.numeric(CONFIRMED_STOREYS),CONFIRMED_UNITS = as.numeric(CONFIRMED_UNITS),SCORE = as.numeric(SCORE),NO_OF_AREAS_EVALUATED = as.numeric(NO_OF_AREAS_EVALUATED),LATITUDE = as.numeric(LATITUDE),LONGITUDE = as.numeric(LONGITUDE),)%>%dplyr::select(id,YEAR_BUILT,YEAR_EVALUATED,PROPERTY_TYPE,CONFIRMED_STOREYS,CONFIRMED_UNITS,SCORE,NO_OF_AREAS_EVALUATED, LATITUDE,LONGITUDE)%>% mutate(Age_Evaluated = YEAR_EVALUATED - YEAR_BUILT)%>% na.omit()

## Create training and test set ##
set.seed(1)
n = nrow(rent)
train <- rent[sample(seq_len(n), size = round(n/2)),]
test <- rent[!rent$id %in% train$id,]
nrow(test)
nrow(train)
```

```{r histogram_rent_clean}
rent_clean = clean_names(rent)
ggplot(data = rent_clean, aes(x = confirmed_units, fill = property_type)) + geom_histogram(position = "dodge") + theme_light() + labs(title = "xxx")
```
```{r}
rent_clean %>% 
  group_by(confirmed_units, property_type) %>% 
  ggplot(aes(confirmed_units, score, color = property_type)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(vars(property_type),
              scales = "free_y"
             )
#> `summarise()` has grouped output by 'line'. You can override
#> using the `.groups` argument.
```

# Confidence Interval 
## Methods 

## Results 

# Multiple Linear Regression 
## Methods

The multiple linear regression (MLR) are used to study the relations between many variables in th form: 

$$y = \beta_0 + \beta_1x_1 + ...+ \beta_px_p+ \epsilon$$
Where $\beta_i$ represents the coefficients need to estiamted, $x_i, i = 1...p$ is the predictor variables, y is the response variable, $\epsilon$ is the error term. 

The assumptions on $\epsilon$ are : 
1. $E(\epsilon) = 0$
2. $V(\epsilon) = \sigma^2I$
3. $\epsilon$ follow normal distribution. 

Under these assumptions, we can get a best linear unbiased estimate (BLUE) by the ordinary least square method. 


If a dataset has p predictors, there would be $2^p$ subset of the predictors in the regression analysis. Thus, when dealing with the dataset will a number of variables, we need to resort to automated selection methods to get a preferred model. Before this procedure, we can use the partial F test to eliminate some variables if we think these variables are unrelated to the research question. There are four measures used as the criteria for selecting models, which are Adjusted $R^2$, Akaike's Information Criterion (AIC), corrected AIC and BIC. In general, we prefer to select a model with a high adjusted $R^2$ while having a small AIC, AICc and BIC. However, sometimes, different models will get by these measures. Thus, the guideline to help us select the model as the best one if it has an adjusted $R^2$ that is slightly smaller but has one of the smallest values of AIC, AICc or BIC and fewer predictors. 

Forward/backward model selection is the most popular stepwise selection method. Forward model selection begins with no predictors and adds one predictor at a time, while backward elimination begins with all the predictors and deletes one predictor at a time. The automated selection method provides us with a systematic way to select a model from a large set of predictors. However, we may not get the best model due to its one-direction operation. 



To validate a model, we need to select an independent dataset from the same population of train dataset and to see if this model has similar performance in the new dataset. We can split the given dataset into two parts: train and test dataset. The proportion to split the dataset can be arbitrary, and 50/50 is the most common split proportion. 

Sometimes we cannot validate our model. Possible reasons for not validating the model include: 1) test dataset is different from training dataset; 2) too many influential points 3) small size of test dataset; 4) specific transformation to correct the model assumption. Thus, we need to compare the distribution of variables in  test dataset before the model validation procedure.



When variable selecting procedure, we need to test the assumptions every times we change the model. However, it will take too much time. Instead, we can assess assumptions with EDA or Residual plots for the full model and the final model sometimes. For example, the histogram of the response variable in EDA can be used to assess the normality assumption. The scatter plot can help us identify the nonlinearity between variables and potential multicollinearity. Residual plots are the most common way to assess violation. Any patterns in the residual plot may suggest some assumption violation of this model. If the non-constant variance, non-linearity and non-normality assumption violate, we can use the transformation method to fix these assumptions. 

## Result


The dataset has 9688 observations and 40 variables, within which some variables record the score for each area. I want to estimate the overall rating of the apartments. Thus, I dropped these variables, and kept 9636 observations and eight variables. I split the dataset randomly by 50/50 to the train and test dataset.  Their summaries are shown in Table 1. The average built year is 1961, and the mean evaluated age is around 60 years. Due to the old age of these buildings, safety issue becomes more crucial. We can find that the train and test dataset have the similar distribution so that we can validate our final model by the test dataset. The pairwise scatter plots are shown in Figure 1, and no apparent nonlinear trend between score and other predictors appears in the figures. There is no very high correlation between covariates for these predictors, so there is no obvious multicollinearity problem. 

```{r train_setup, include = FALSE}
rent = train
summary(rent)
table(rent$PROPERTY_TYPE)
table(rent$PROPERTY_TYPE)/length(rent$PROPERTY_TYPE)

sd(rent$YEAR_BUILT)
sd(rent$YEAR_EVALUATED)
sd(rent$CONFIRMED_STOREYS)
sd(rent$CONFIRMED_UNITS)
sd(rent$SCORE)
sd(rent$NO_OF_AREAS_EVALUATED)
sd(rent$Age_Evaluated)
sd(rent$LATITUDE)
sd(rent$LONGITUDE)
```



```{r train_num,  fig.width=8, fig.height=8,fig.cap="Scatter plot for the train dataset",echo = FALSE}
train_num = train%>%dplyr::select(SCORE,CONFIRMED_STOREYS,CONFIRMED_UNITS,NO_OF_AREAS_EVALUATED, LATITUDE,LONGITUDE,Age_Evaluated)
plot(train_num)
```


**Table 1: Summary results of test and train dataset**

|  | Train Dataset|Test Dataset|
|--------------- |--------------- | --------------- | 
|**Built Year** |
|Mean/SD| 1961/18.46|1961/18.37|
|**Evaluated Year** |
|Mean/SD| 2018/1.47 | 2018/1.45|
|**Evaluated Age** |
|Mean/SD| 57.6/18.5 |57.9/18.4 |
|**Storeys** |
|Mean/SD| 7.6/6.1 |7.4/6.0|
|**Units** |
|Mean/SD| 88/94 |86/93 |
|**No of Evaluated Areas** |
|Mean/SD| 17.2/1.66 |17.1/1.67 |
|**Evaluated Score** |
|Mean/SD| 71.2/10 | 72.2/10 |
|**LATITUDE** |
|Mean/SD| 43.7/0.04 | 43.7/0.04 |
|**LONGITUDE** |
|Mean/SD| -79.4/0.09 |-79.4/0.09 |
|**Property type** |
|Private| 4014(84%) |4004(84%) |
|Housing| 289(6%)| 293(6%)|
|TCHC| 455(10%) |461(10%) |
|**Sample size** |4758 |4758|



## Process of Obtaining Final model
Firstly, I run the full regrssion model and the results are shown in the first column of Table 2. The variance inflation factor (VIF) are all less than 5, so no multicolinearity occurs. The LATITUDE is not significant and there are 7 predictors in the model, so I choose automated selection method by AIC and BIC measures, and the results are shown in column 2 and 3. The AIC model is the same as the full model, while the BIC model drops confirmed storeys, confirmed units and Latitude. The $R^2$ of BIC model is 0.078, only 0.2% smaller than the full model, and it has the smallest BIC measure. So I select the BIC model as my final model in the end. 


```{r full_model}
full_model = lm(SCORE~Age_Evaluated+CONFIRMED_STOREYS+CONFIRMED_UNITS+NO_OF_AREAS_EVALUATED+as.factor(PROPERTY_TYPE)+ LATITUDE+LONGITUDE,data = train )
AIC_model = step(full_model)
n = nrow(train)
BIC_model = step(full_model,k = log(n))
```




```{r test_BIC}
Test_BIC_model = lm(SCORE ~ Age_Evaluated + NO_OF_AREAS_EVALUATED + as.factor(PROPERTY_TYPE) + LONGITUDE, data = test)
```


```{r huxtable}
huxtable::huxreg(full_model,AIC_model,BIC_model,Test_BIC_model)
```

```{r vic_full_model}
library(regclass)
VIF(full_model)
```

## Goodness of Final model
From Figure 1, the additional conditions have been satisfied. The redisual plots for BIC model are shown in Figure2. The top-left figure shows that linearity and constant variance assumption are satisfied. The normal Q-Q plot shows the normality assumption is also hold. Then, the bottom two figures show that no obvious outliers and influntial points, but there are several leverage points. In all, all the assumptions are good, so we don't need to do further transformation. 

I validate the final model by the test dataset, and the results are shown in the last column of Table 2. The estimated coefficients between test dataset and train dataset have the similar sign and significance. But, we still find some difference, for example, the R2 of final model in train dataset is 7.8%, while the one in test dataset is 10.4%. The p-value of the coefficeint of age is less than 0.001, while it is between 0.01 and 0.05 in the test dataset. 


```{r bic_plot,  fig.width=6, fig.height=6,fig.cap="Residual plots",echo = FALSE}
par(mfrow = c(2,2))
plot(BIC_model)
```


# Deploy API Model 
## Methods 
## Results 

# Discussions 

# Appedix